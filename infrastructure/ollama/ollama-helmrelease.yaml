apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: ollama
spec:
  chart:
    spec:
      chart: ollama
      sourceRef:
        kind: HelmRepository
        name: ollama
      version: "1.42.0"
  interval: 1h0m0s
  values:
    image:
      repository: ollama/ollama
      # See: https://github.com/ollama/ollama/issues/14086
      tag: "0.15.5-rc1"
    ollama:
      port: 11434
      gpu:
        enabled: true
        type: nvidia
        number: 1
      models:
        pull:
          - codestral:22b
          - deepseek-r1:8b
          - embeddinggemma:300m
          - gemma3:4b
          - glm-4.7-flash:q4_K_M
          - gpt-oss:20b
          - llama3.1:8b
          - llama3.2:3b
          - mistral:7b
          - qwen2.5-coder:1.5b
          - qwen2.5-coder:7b
          - qwen3-coder-next:q4_K_M
          - qwen3-coder:30b
          - qwen3-embedding:8b
          - qwen3-vl:32b
          - qwen3-vl:4b
          - qwen3-vl:4b-instruct
          - qwen3-vl:8b
          - qwen3-vl:8b-instruct
          - qwen3:4b-instruct
          - qwen3:8b
          - translategemma:4b
        create:
        - name: qwen3-coder-next:IQ1_M
          template: |
            FROM hf.co/unsloth/Qwen3-Coder-Next-GGUF:IQ1_M
        run: []
        clean: true
    service:
      type: ClusterIP
      port: 11434
    resources:
      requests:
        cpu: 1
        memory: 4Gi
      limits:
        # memory: 24Gi
        nvidia.com/gpu: "1"
    # See: https://github.com/ollama/ollama/blob/main/envconfig/config.go
    extraEnv:
      - name: OLLAMA_DEBUG
        value: "1"
      # See: https://docs.ollama.com/faq#how-does-ollama-handle-concurrent-requests ------
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "0"
      - name: OLLAMA_NUM_PARALLEL
        value: "3"
      - name: OLLAMA_MAX_QUEUE
        value: "512"
      # ------------------------------------------------------------------------------
      - name: OLLAMA_KEEP_ALIVE
        value: "10m"
      - name: OLLAMA_LOAD_TIMEOUT
        value: "5m"
      # See: https://docs.ollama.com/context-length
      - name: OLLAMA_CONTEXT_LENGTH
        value: "16384"
      - name: OLLAMA_MULTIUSER_CACHE
        value: "false"
      - name: OLLAMA_NOPRUNE
        value: "false"
      - name: OLLAMA_NOHISTORY
        value: "false"
      # See: https://docs.ollama.com/faq#how-can-i-enable-flash-attention
      - name: OLLAMA_FLASH_ATTENTION
        value: "true"
      # See: https://docs.ollama.com/faq?utm_source=chatgpt.com#how-can-i-set-the-quantization-type-for-the-k/v-cache
      - name: OLLAMA_KV_CACHE_TYPE
        value: "q8_0"
      - name: OLLAMA_SCHED_SPREAD
        value: "false"
      - name: OLLAMA_NEW_ENGINE
        value: "true"
    persistentVolume:
      enabled: true
      size: 300Gi
      storageClass: topolvm
    runtimeClassName: nvidia
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: nvidia.com/gpu.present
              operator: Exists
    tolerations:
    - effect: NoSchedule
      key: node.mmontes.io/type
      value: compute-xlarge