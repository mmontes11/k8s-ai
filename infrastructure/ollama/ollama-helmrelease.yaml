apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: ollama
spec:
  chart:
    spec:
      chart: ollama
      sourceRef:
        kind: HelmRepository
        name: ollama
      version: "1.36.0"
  interval: 1h0m0s
  values:
    ollama:
      port: 11434
      models:
        pull:
          - codestral:22b
          - deepseek-r1:8b
          - gemma3:4b
          - gpt-oss:20b
          - llama3.1:8b
          - llama3.2:3b
          - mistral:7b
          - qwen2.5-coder:1.5b
          - qwen2.5-coder:7b
          - qwen3-coder:30b
          - qwen3-vl:4b
          - qwen3-vl:4b-instruct
          - qwen3-vl:8b
          - qwen3-vl:8b-instruct
          - qwen3:4b-instruct
          - qwen3:8b
        run: []
    service:
      type: ClusterIP
      port: 11434
    resources:
      requests:
        cpu: 4
        memory: 8Gi
      limits:
        memory: 32Gi
    # See: https://github.com/ollama/ollama/blob/main/envconfig/config.go
    extraEnv:
      - name: OLLAMA_DEBUG
        value: "1"
      - name: OLLAMA_MAX_QUEUE
        value: "100"
      - name: OLLAMA_NUM_PARALLEL
        value: "1"
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "1"
      - name: OLLAMA_KEEP_ALIVE
        value: "5m"
      - name: OLLAMA_LOAD_TIMEOUT
        value: "5m"
      - name: OLLAMA_CONTEXT_LENGTH
        value: "65536"
      - name: OLLAMA_MULTIUSER_CACHE
        value: "false"
      - name: OLLAMA_NOPRUNE
        value: "false"
      - name: OLLAMA_NOHISTORY
        value: "false"
      - name: OLLAMA_KV_CACHE_TYPE
        value: "f16"
      - name: OLLAMA_SCHED_SPREAD
        value: "false"
      - name: OLLAMA_NEW_ENGINE
        value: "true"
    persistentVolume:
      enabled: true
      size: 300Gi
      storageClass: topolvm
    tolerations:
    - effect: NoSchedule
      key: node.mmontes.io/type
      value: compute-xlarge
    nodeSelector:
      node.mmontes.io/type: compute-xlarge