apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: ollama
spec:
  chart:
    spec:
      chart: ollama
      sourceRef:
        kind: HelmRepository
        name: ollama
      version: "1.38.0"
  interval: 1h0m0s
  values:
    image:
      repository: ollama/ollama
      tag: "0.14.3"
    ollama:
      port: 11434
      models:
        pull:
          - codestral:22b
          - deepseek-r1:8b
          - embeddinggemma:300m
          - gemma3:4b
          - glm-4.7-flash:q4_K_M
          - gpt-oss:20b
          - llama3.1:8b
          - llama3.2:3b
          - mistral:7b
          - qwen2.5-coder:1.5b
          - qwen2.5-coder:7b
          - qwen3-coder:30b
          - qwen3-embedding:8b
          - qwen3-vl:32b
          - qwen3-vl:4b
          - qwen3-vl:4b-instruct
          - qwen3-vl:8b
          - qwen3-vl:8b-instruct
          - qwen3:4b-instruct
          - qwen3:8b
          - translategemma:4b
        run: []
    service:
      type: ClusterIP
      port: 11434
    resources:
      requests:
        cpu: 4
        memory: 8Gi
      limits:
        memory: 24Gi
    # See: https://github.com/ollama/ollama/blob/main/envconfig/config.go
    extraEnv:
      - name: OLLAMA_DEBUG
        value: "1"
      # See: https://docs.ollama.com/faq#how-does-ollama-handle-concurrent-requests
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "3"
      - name: OLLAMA_NUM_PARALLEL
        value: "3"
      - name: OLLAMA_MAX_QUEUE
        value: "100"
      # ---
      - name: OLLAMA_KEEP_ALIVE
        value: "5m"
      - name: OLLAMA_LOAD_TIMEOUT
        value: "5m"
      - name: OLLAMA_CONTEXT_LENGTH
        value: "32768"
      - name: OLLAMA_MULTIUSER_CACHE
        value: "false"
      - name: OLLAMA_NOPRUNE
        value: "false"
      - name: OLLAMA_NOHISTORY
        value: "false"
      - name: OLLAMA_KV_CACHE_TYPE
        value: "f16"
      - name: OLLAMA_SCHED_SPREAD
        value: "false"
      - name: OLLAMA_NEW_ENGINE
        value: "true"
    persistentVolume:
      enabled: true
      size: 300Gi
      storageClass: topolvm
    gpu:
      enabled: true
      type: nvidia
      number: 1
    runtimeClassName: nvidia
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: nvidia.com/gpu.present
              operator: Exists
    tolerations:
    - effect: NoSchedule
      key: node.mmontes.io/type
      value: compute-xlarge